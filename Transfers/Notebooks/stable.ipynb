{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7024a27f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#data loading\n",
    "import pandas as pd\n",
    "predata = pd.read_csv('res.csv', low_memory=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2ad3a1c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#-------------------------------------------------pre-processing--------------------------------------------------------------#\n",
    "\n",
    "#begin of preprocessing\n",
    "import time\n",
    "\n",
    "start = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "004e6bcc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/g7/nkrn8hv17wd0kycz8srzhf7r0000gn/T/ipykernel_11435/105919031.py:58: FutureWarning: Setting categories in-place is deprecated and will raise in a future version. Use rename_categories instead.\n",
      "  x.categories = labels_a\n",
      "/var/folders/g7/nkrn8hv17wd0kycz8srzhf7r0000gn/T/ipykernel_11435/105919031.py:58: FutureWarning: Setting categories in-place is deprecated and will raise in a future version. Use rename_categories instead.\n",
      "  x.categories = labels_a\n"
     ]
    }
   ],
   "source": [
    "#convert amount and accountbalance to classes and assign a word to each interval  \n",
    "import numpy as np\n",
    "\n",
    "#automatic labels\n",
    "import string\n",
    "\n",
    "\n",
    "class LabelCategorizer:\n",
    "    def __init__(self, base_word='cat'):\n",
    "        self.initial = 1\n",
    "        self._alphabet_index = 0\n",
    "        self.base_word = base_word\n",
    "        self.current_word = self.base_word\n",
    "        self.shift = 0\n",
    "\n",
    "    def __str__(self):\n",
    "        return 'Class: Label Categorizer\\nBase word: ' + self.base_word + '\\nCurrent Word: ' + self.current_word\n",
    "\n",
    "    def get_next_word(self):\n",
    "        if self.initial:\n",
    "            self.initial = 0\n",
    "            return self.current_word\n",
    "\n",
    "        if self.shift > 0:\n",
    "            self.current_word = self.current_word[-1] + self.current_word[:-1]\n",
    "            self.shift -= 1\n",
    "        else:\n",
    "            self.current_word = self.current_word + string.ascii_lowercase[self._alphabet_index]\n",
    "            self._alphabet_index = (self._alphabet_index + 1) % len(string.ascii_lowercase)\n",
    "            self.shift = len(self.current_word) - 1\n",
    "\n",
    "        return self.current_word\n",
    "\n",
    "\n",
    "#replacement of the old columns with the new ones with classes\n",
    "def cutter(col, number, word, words_map):\n",
    "    #make sure that only positives are assigned an interval\n",
    "    col_min = max(predata[col].min(), 1)\n",
    "    col_max = max(predata[col].max(), 1)\n",
    "\n",
    "    bins_a = np.geomspace(float(col_min), float(col_max), num=number)\n",
    "    bins_a[0] = bins_a[0] - 1\n",
    "    bins_aux = bins_a[1:]\n",
    "    bins_aux = np.append(bins_aux, bins_a[-1] + 1)\n",
    "    bin_tuples = list(zip(bins_a, bins_aux))\n",
    "\n",
    "    bins = pd.IntervalIndex.from_tuples(bin_tuples)\n",
    "\n",
    "    #range of the intervals made\n",
    "    labels_a = []\n",
    "\n",
    "    a = LabelCategorizer(base_word=word)\n",
    "\n",
    "    for _ in range(number):\n",
    "        labels_a.append(a.get_next_word())\n",
    "\n",
    "    x = pd.cut(predata[col].to_list(), bins=bins)\n",
    "    x.categories = labels_a\n",
    "    predata[col] = x\n",
    "\n",
    "    for i in range(number):\n",
    "        words_map[labels_a[i]] = bins[i]\n",
    "\n",
    "    #columns to apply the conversion\n",
    "\n",
    "\n",
    "columns = ['amount', 'accountbalance']\n",
    "\n",
    "#number of intervals for each column\n",
    "number_bins = [36, 38]\n",
    "\n",
    "#base words assigned to each column on columns to apply the conversion\n",
    "base_words = ['pink', 'red']\n",
    "\n",
    "#get acess to the range of the interval based on the word that appears\n",
    "values_map = {}\n",
    "\n",
    "for i in range(len(columns)):\n",
    "    cutter(columns[i], number_bins[i], base_words[i], values_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "359d2c35",
   "metadata": {},
   "outputs": [],
   "source": [
    "#bins amount\n",
    "#transfers: 1M - 35.8775 / 2M - 37.3285 / 3M - 38.2194 / 4M - 38.818 / 5M - 39.3326 / gans - 26.3159 / imbalanced - 24.7233\n",
    "#payments: 1M - 36.7452 / 2M - 38.2134 / 3M - 39.1709 / 4M - 39.8368 / 5M - 40.2234 / gans - 24.8089 / imbalanced - 22.5533\n",
    "\n",
    "#bins accountbalance\n",
    "#transfers: 1M - 37.0555 / 2M - 38.5003 / 3M - 39.4439 / 4M - 40.1224 / 5M - 40.627 / gans - 25.734 / imbalanced - 23.9742\n",
    "#payments: 1M - 37.234 / 2M - 38.6645 / 3M - 39.651 / 4M - 40.2506 / 5M - 40.7402 / gans - 26.0935 / imbalanced - 20.7986"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "26ac74c5",
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#assign the word negaccount for negative values of accountbalance  \n",
    "aux = predata['accountbalance'].values\n",
    "\n",
    "vacc = []\n",
    "\n",
    "for elm in aux:\n",
    "    if str(elm) == 'nan':\n",
    "        vacc.append('negaccount')\n",
    "    else:\n",
    "        vacc.append(elm)\n",
    "predata['accountbalance'] = vacc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "405df442",
   "metadata": {},
   "outputs": [],
   "source": [
    "#interval that a word corresponds to\n",
    "#values_map['red']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "895acc61",
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert hours to classes and assign a word to each interval  \n",
    "bins_hour = [0, 4, 8, 12, 16, 20, 24]\n",
    "\n",
    "#labels assigned to each interval\n",
    "labels_hour = ['dawn', 'earlymorning', 'morning', 'afternoon', 'dusk', 'night']\n",
    "\n",
    "predata['hour'] = list(\n",
    "    pd.cut(predata['hour'], bins=bins_hour, labels=labels_hour, retbins=True, include_lowest=True)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d0427d44",
   "metadata": {},
   "outputs": [],
   "source": [
    "#map trusted_indicator values to words\n",
    "ti_value_map = {\n",
    "    '0.0': 'ntrusted',\n",
    "    '1.0': 'trusted',\n",
    "    'unknown': 'tunknown'\n",
    "}\n",
    "predata['trusted_indicator'] = predata['trusted_indicator'].apply(lambda x: ti_value_map[x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6815fe4a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#add letter before number to distinguish between similar numbers from different columns\n",
    "cols = ['entity', 'reference', 'iban_orig', 'iban_dest', 'ipaddress', 'clientid', 'week']\n",
    "\n",
    "identifier = ['e', 'r', 'io', 'id', 'ip', 'c', 'w']\n",
    "\n",
    "for col in range(len(cols)):\n",
    "    predata[cols[col]] = predata[cols[col]].apply(lambda x: identifier[col] + str(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6e3b022d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#convert binary and chains of numbers to specific words\n",
    "def apply_map(df, col, target_map):\n",
    "    df[col] = df[col].apply(lambda x: target_map.get(str(x)))\n",
    "\n",
    "\n",
    "cols_maps = [('is_fraud', {'0': 'nfraud', '1': 'fraud'}),\n",
    "             ('weekday', {'0': 'mon', '1': 'tue', '2': 'wed', '3': 'thu', '4': 'fri', '5': 'sat', '6': 'sun'}),\n",
    "             ('month', {'1': 'jan', '2': 'feb', '3': 'mar', '4': 'apr', '5': 'may', '6': 'jun', '7': 'jul', '8': 'aug',\n",
    "                        '9': 'sep',\n",
    "                        '10': 'oct', '11': 'nov', '12': 'dec'})]\n",
    "\n",
    "for comb in cols_maps:\n",
    "    apply_map(predata, comb[0], comb[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "20c540cd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#make fraud column as the center column\n",
    "new_order = ['canal', 'operativa', 'clientid', 'entity', 'reference', 'trusted_indicator', 'iban_orig', 'iban_dest',\n",
    "             'amount', 'is_fraud', 'accountbalance', 'ipaddress', 'browser_family', 'os_family', 'hour', 'week', 'weekday',\n",
    "             'month', 'device']\n",
    "\n",
    "predata = predata[new_order]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "add62c42",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#select data for train and test  \n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "#target column\n",
    "y = predata['is_fraud']\n",
    "predata.drop('is_fraud', axis=1)\n",
    "\n",
    "#train and test\n",
    "X_train, X_test, y_train, y_test = train_test_split(predata, y, stratify=y, test_size=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4357c1c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#function for conversion format\n",
    "def convert_to_w2v_format(df):\n",
    "    sentences = df.to_numpy()\n",
    "    sentences_aux = [list(curr) for curr in sentences]\n",
    "    sentences_series = pd.Series(sentences_aux)\n",
    "    return sentences_series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9db10e74",
   "metadata": {},
   "outputs": [],
   "source": [
    "#for entering the model\n",
    "sentences_series = convert_to_w2v_format(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c4ad421a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#for the test metrics\n",
    "X_test_np = X_test.copy()\n",
    "sentences_series_np_test = convert_to_w2v_format(X_test_np)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "bba911c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#for the train metrics\n",
    "X_train_np = X_train.copy()\n",
    "sentences_series_np_train = convert_to_w2v_format(X_train_np)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b41ed2bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This corpus contains 13300000 tokens\n"
     ]
    }
   ],
   "source": [
    "#get size of the corpus \n",
    "token_count = sum([len(sentence) for sentence in sentences_series])\n",
    "\n",
    "print(\"This corpus contains {} tokens\".format(token_count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "944ead17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training time: 10.665153741836548s\n"
     ]
    }
   ],
   "source": [
    "#end of preprocessing\n",
    "stop = time.time()\n",
    "\n",
    "print(f\"Training time: {stop - start}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f8fa04ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "#-----------------------------------------------------training----------------------------------------------------------------#\n",
    "\n",
    "#begin of training\n",
    "begin = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1640ad3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#callback to print loss after each epoch\n",
    "import gensim.models.word2vec as w2v\n",
    "from gensim.models.callbacks import CallbackAny2Vec\n",
    "from mlflow import log_metric\n",
    "\n",
    "class callback(CallbackAny2Vec):\n",
    "\n",
    "    def __init__(self):\n",
    "        self.epoch = 0\n",
    "        self.loss_to_be_subed = 0\n",
    "\n",
    "    def on_epoch_end(self, model):\n",
    "        loss = model.get_latest_training_loss()\n",
    "        loss_now = loss - self.loss_to_be_subed\n",
    "        self.loss_to_be_subed = loss\n",
    "        print('Loss after epoch {}: {}'.format(self.epoch, loss_now))\n",
    "        self.epoch += 1\n",
    "\n",
    "        log_metric('Loss', loss_now, step=self.epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f7ae36bf",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#inicialization and training word2vec \n",
    "import multiprocessing\n",
    "\n",
    "def training(sentences, cycles, dim, window, sample, negative, hs, exponent, alpha, min_alpha):\n",
    "    model = w2v.Word2Vec(\n",
    "        sg=1,  #skip-gram - fixed\n",
    "        workers=multiprocessing.cpu_count(),  #use all cores - fixed\n",
    "        vector_size=dim,  #dimension of the embedding space - change\n",
    "        window=window,  #words befores and after the center word - change\n",
    "        sample=sample,  #whithout subsampling - change\n",
    "        min_count=1,  #use every word - fixed\n",
    "        negative=negative,  #noise-words - change\n",
    "        hs=hs,  #negative sampling\n",
    "        ns_exponent=exponent,  #exponent to shape negative sampling - change\n",
    "        alpha=alpha,  #initial learning rate - change\n",
    "        min_alpha=min_alpha  #final learning rate - change\n",
    "    )\n",
    "\n",
    "    #vocabulary creation\n",
    "    model.build_vocab(sentences)\n",
    "\n",
    "    #model training\n",
    "    model.train(sentences, epochs=cycles, total_examples=model.corpus_count, compute_loss=True, callbacks=[callback()])\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "#model creation\n",
    "# model = training(sentences_series, 10, 5, 9, 0, 5, 0, 0.75, 0.025, 0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b04bbcf3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#info about the trained model \n",
    "# print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a221511",
   "metadata": {},
   "outputs": [],
   "source": [
    "#model's memory consuming members with their size in bytes\n",
    "#model.estimate_memory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1071262c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#saving the model\n",
    "#model.save(r'C:/Users/BeatrizCarvalho/OneDrive - Closer Consultoria Lda/Documents/Entangled-Spaces/Datasets/3transfers_word2vec_matrix_originalcols.w2v')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e941d7f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training time: 0.3412210941314697s\n"
     ]
    }
   ],
   "source": [
    "#end of training\n",
    "end = time.time()\n",
    "\n",
    "print(f\"Training time: {end - begin}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88c874e8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#------------------------------------------probability vector for a client----------------------------------------------------#\n",
    "\n",
    "#weight matrices\n",
    "# m1 = model.wv.vectors\n",
    "# m2 = model.syn1neg  #negative sampling\n",
    "#m2 = model.syn1       #hierarchical-softmax "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "37abf548",
   "metadata": {},
   "outputs": [],
   "source": [
    "#update probv - vector with the parameters of a transaction\n",
    "from scipy.special import softmax\n",
    "\n",
    "def predictTransaction(my_model, m_cid, my_curr_cid, verbose):\n",
    "    #obtain the representative vector of the clientid\n",
    "    civ = my_model.wv.get_vector(m_cid)\n",
    "\n",
    "    #multiplication of the clientid vector by the decode matrix (M2)  \n",
    "    m2 = my_model.syn1neg\n",
    "    \n",
    "    #vector of len = vocab_size that softmax is applied to obtain the conditional probabilities \n",
    "    vout = softmax(np.dot(civ, m2.T))\n",
    "\n",
    "    fraud_curr = my_curr_cid.copy()\n",
    "    fraud_curr.append(my_model.wv.index_to_key.index('fraud'))\n",
    "\n",
    "    nfraud_curr = my_curr_cid.copy()\n",
    "    nfraud_curr.append(my_model.wv.index_to_key.index('nfraud'))\n",
    "\n",
    "    #filter the vector to the transaction fields\n",
    "    fraud_probv = sum(vout[fraud_curr])\n",
    "    nfraud_probv = sum(vout[nfraud_curr])\n",
    "\n",
    "    #print results if verbose True\n",
    "    if verbose:\n",
    "        print(f\"Transaction: {my_curr_cid}\\nFraud prob: {fraud_probv}\\nNon Fraud prob: {nfraud_probv}\")\n",
    "        # https://www.geeksforgeeks.org/writing-to-file-in-python/#appending \n",
    "        # print(f\"Transaction Nº {i} \\t Sum: {fprob} \\n\")\n",
    "\n",
    "    # if fraud_probv > threshold: return 1\n",
    "    # If the probability \n",
    "\n",
    "    if fraud_probv > nfraud_probv:\n",
    "        return 1\n",
    "    \n",
    "    return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "724da724",
   "metadata": {},
   "outputs": [],
   "source": [
    "#prediction method   \n",
    "def predict(model, X, threshold, verbose1, verbose2):\n",
    "    #save the predictions made by the model in a list\n",
    "    predictions = []\n",
    "\n",
    "    #for each transaction (eval_row)\n",
    "    for i, eval_row in enumerate(X, 1):\n",
    "\n",
    "        #print transaction parameters if verbose1 True\n",
    "        if verbose1:\n",
    "            print(eval_row)\n",
    "\n",
    "        #cid is always in position 2 of the array\n",
    "        curr_cid = eval_row[2]\n",
    "\n",
    "        #for each sentence_series creates a current list\n",
    "        curr = []\n",
    "\n",
    "        #if curr_cid is not known\n",
    "        if curr_cid not in model.wv.index_to_key:\n",
    "            #update model\n",
    "            model.build_vocab([[curr_cid]], update=True)\n",
    "\n",
    "        #filter the parameters associated with the transaction made by the specific clientid (civ)\n",
    "        for x in eval_row:\n",
    "            if x != curr_cid:\n",
    "                if x in model.wv.index_to_key:\n",
    "                    curr.append(model.wv.index_to_key.index(x))\n",
    "\n",
    "                else:\n",
    "                    #if the word is not known                        \n",
    "                    #update model\n",
    "                    model.build_vocab([[x]], update=True)\n",
    "                    curr.append(model.wv.index_to_key.index(x))\n",
    "\n",
    "        #sum each value on the array to obtain the final probability\n",
    "        curr_prediction = predictTransaction(model, curr_cid, curr, verbose2)\n",
    "        predictions.append(curr_prediction)\n",
    "\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5eb36784",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#apply the prediction method for the train set\n",
    "# predict(model, sentences_series_np_train, 0.5, verbose1=True, verbose2=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d360ed11",
   "metadata": {},
   "outputs": [],
   "source": [
    "#true values\n",
    "# y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "4eeec16e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert true values in train set to binary\n",
    "y_train = [1 if elem == \"fraud\" else 0 for elem in y_train]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b7f94ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "#metrics for the train set\n",
    "from imblearn.metrics import geometric_mean_score\n",
    "from sklearn.metrics import precision_recall_fscore_support, accuracy_score, matthews_corrcoef, confusion_matrix, precision_recall_curve\n",
    "\n",
    "trs = y_train[:501]\n",
    "prevs = predict(model, X_train_np.values[:501], 0.1, verbose1=False, verbose2=False, shouldBe=trs)\n",
    "\n",
    "#accuracy\n",
    "accuracy = accuracy_score(trs,prevs)\n",
    "print('accuracy: {}'.format(accuracy))\n",
    "\n",
    "#precision, recall, f-score\n",
    "precision, recall, fscore, support = precision_recall_fscore_support(trs, prevs, average='micro')\n",
    "\n",
    "#precision - ratio tp / (tp + fp) - ability not to label a negative sample as positive\n",
    "print('precision: {}'.format(precision))\n",
    "\n",
    "#recall - ratio tp / (tp + fn) - ability to find all the positive samples - best is 1, worst is 0\n",
    "print('recall: {}'.format(recall))\n",
    "\n",
    "#fscore - weighted harmonic mean of the precision and recall - best is 1, worst is 0\n",
    "print('fscore: {}'.format(fscore))\n",
    "\n",
    "#matthews correlation coefficient - measure of the quality of binary classifications\n",
    "#can be used even if the classes are of very different sizes - is in essence a correlation coefficient between -1 and +1\n",
    "#+1 means perfect prediction, 0 an average random prediction, -1 an inverse prediction\n",
    "mcc = matthews_corrcoef(trs, prevs)\n",
    "print('mcc: {}'.format(mcc))\n",
    "\n",
    "#g-mean - squared root of the product of the sensitivity and specificity - best is 1, worst is 0\n",
    "print('G-mean:', (geometric_mean_score(trs, prevs,average='micro')))\n",
    "\n",
    "#specificity - ability to predict true negatives of each available category - recall of the negative class\n",
    "#specifity = tn / (tn + fp)\n",
    "#print('specifity: {}'.format(specifity))\n",
    "\n",
    "#sensitivity - ability to predict true positives of each available category = recall\n",
    "\n",
    "#confusion matrix    \n",
    "print(confusion_matrix(trs, prevs))\n",
    "\n",
    "#true positives, false positives, true negatives, false negatives\n",
    "tn, fp, fn, tp = confusion_matrix(trs,prevs).ravel()\n",
    "\n",
    "#true negatives\n",
    "print('true negatives: {}'.format(tn))\n",
    "\n",
    "#false positives\n",
    "print('false positives: {}'.format(fp))\n",
    "\n",
    "#false negatives\n",
    "print('false negatives: {}'.format(fn))\n",
    "\n",
    "#false positives\n",
    "print('true positives: {}'.format(tp))\n",
    "\n",
    "#error rate\n",
    "error_rate = 1 - accuracy\n",
    "print('error rate: {}'.format(error_rate))\n",
    "\n",
    "#precision-recall curve - compute precision-recall pairs for different probability thresholds\n",
    "print(precision_recall_curve(trs, prevs))\n",
    "\n",
    "#roc curve\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import metrics\n",
    "\n",
    "fpr, tpr, thresholds = metrics.roc_curve(trs,prevs)\n",
    "\n",
    "roc_auc = metrics.auc(fpr, tpr)\n",
    "\n",
    "display = metrics.RocCurveDisplay(fpr=fpr, tpr=tpr, roc_auc=roc_auc, estimator_name='Word2vec')\n",
    "display.plot()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "71b305e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss after epoch 0: 20840684.0\n",
      "Loss after epoch 1: 12059090.0\n",
      "Loss after epoch 2: 7994422.0\n",
      "Loss after epoch 3: 6859692.0\n",
      "Loss after epoch 4: 5689196.0\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [26], line 34\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[39mfor\u001b[39;00m h \u001b[39min\u001b[39;00m hs:\n\u001b[1;32m     33\u001b[0m     i\u001b[39m+\u001b[39m\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m\n\u001b[0;32m---> 34\u001b[0m     \u001b[39mwith\u001b[39;00m mlflow\u001b[39m.\u001b[39mstart_run(nested\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m):\n\u001b[1;32m     35\u001b[0m \n\u001b[1;32m     36\u001b[0m         \u001b[39m# Log params to mlflow\u001b[39;00m\n\u001b[1;32m     37\u001b[0m         log_param(\u001b[39m\"\u001b[39m\u001b[39mcycles\u001b[39m\u001b[39m\"\u001b[39m, c)\n\u001b[1;32m     38\u001b[0m         log_param(\u001b[39m\"\u001b[39m\u001b[39mdim\u001b[39m\u001b[39m\"\u001b[39m, d)\n",
      "Cell \u001b[0;32mIn [26], line 34\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[39mfor\u001b[39;00m h \u001b[39min\u001b[39;00m hs:\n\u001b[1;32m     33\u001b[0m     i\u001b[39m+\u001b[39m\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m\n\u001b[0;32m---> 34\u001b[0m     \u001b[39mwith\u001b[39;00m mlflow\u001b[39m.\u001b[39mstart_run(nested\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m):\n\u001b[1;32m     35\u001b[0m \n\u001b[1;32m     36\u001b[0m         \u001b[39m# Log params to mlflow\u001b[39;00m\n\u001b[1;32m     37\u001b[0m         log_param(\u001b[39m\"\u001b[39m\u001b[39mcycles\u001b[39m\u001b[39m\"\u001b[39m, c)\n\u001b[1;32m     38\u001b[0m         log_param(\u001b[39m\"\u001b[39m\u001b[39mdim\u001b[39m\u001b[39m\"\u001b[39m, d)\n",
      "File \u001b[0;32m~/Desktop/issueBea/issueVenv/lib/python3.10/site-packages/debugpy/_vendored/pydevd/_pydevd_bundle/pydevd_frame.py:1183\u001b[0m, in \u001b[0;36mPyDBFrame.trace_dispatch\u001b[0;34m(self, frame, event, arg)\u001b[0m\n\u001b[1;32m   1181\u001b[0m \u001b[39mif\u001b[39;00m is_line:\n\u001b[1;32m   1182\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mset_suspend(thread, step_cmd, original_step_cmd\u001b[39m=\u001b[39minfo\u001b[39m.\u001b[39mpydev_original_step_cmd)\n\u001b[0;32m-> 1183\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdo_wait_suspend(thread, frame, event, arg)\n\u001b[1;32m   1184\u001b[0m \u001b[39melif\u001b[39;00m is_return:  \u001b[39m# return event\u001b[39;00m\n\u001b[1;32m   1185\u001b[0m     back \u001b[39m=\u001b[39m frame\u001b[39m.\u001b[39mf_back\n",
      "File \u001b[0;32m~/Desktop/issueBea/issueVenv/lib/python3.10/site-packages/debugpy/_vendored/pydevd/_pydevd_bundle/pydevd_frame.py:164\u001b[0m, in \u001b[0;36mPyDBFrame.do_wait_suspend\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    163\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdo_wait_suspend\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m--> 164\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_args[\u001b[39m0\u001b[39;49m]\u001b[39m.\u001b[39;49mdo_wait_suspend(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/Desktop/issueBea/issueVenv/lib/python3.10/site-packages/debugpy/_vendored/pydevd/pydevd.py:2062\u001b[0m, in \u001b[0;36mPyDB.do_wait_suspend\u001b[0;34m(self, thread, frame, event, arg, exception_type)\u001b[0m\n\u001b[1;32m   2059\u001b[0m             from_this_thread\u001b[39m.\u001b[39mappend(frame_custom_thread_id)\n\u001b[1;32m   2061\u001b[0m     \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_threads_suspended_single_notification\u001b[39m.\u001b[39mnotify_thread_suspended(thread_id, stop_reason):\n\u001b[0;32m-> 2062\u001b[0m         keep_suspended \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_do_wait_suspend(thread, frame, event, arg, suspend_type, from_this_thread, frames_tracker)\n\u001b[1;32m   2064\u001b[0m frames_list \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m   2066\u001b[0m \u001b[39mif\u001b[39;00m keep_suspended:\n\u001b[1;32m   2067\u001b[0m     \u001b[39m# This means that we should pause again after a set next statement.\u001b[39;00m\n",
      "File \u001b[0;32m~/Desktop/issueBea/issueVenv/lib/python3.10/site-packages/debugpy/_vendored/pydevd/pydevd.py:2098\u001b[0m, in \u001b[0;36mPyDB._do_wait_suspend\u001b[0;34m(self, thread, frame, event, arg, suspend_type, from_this_thread, frames_tracker)\u001b[0m\n\u001b[1;32m   2095\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_call_input_hook()\n\u001b[1;32m   2097\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprocess_internal_commands()\n\u001b[0;32m-> 2098\u001b[0m     time\u001b[39m.\u001b[39;49msleep(\u001b[39m0.01\u001b[39;49m)\n\u001b[1;32m   2100\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcancel_async_evaluation(get_current_thread_id(thread), \u001b[39mstr\u001b[39m(\u001b[39mid\u001b[39m(frame)))\n\u001b[1;32m   2102\u001b[0m \u001b[39m# process any stepping instructions\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#grid-search\n",
    "#save results\n",
    "import os\n",
    "from mlflow import log_metric, log_param, log_artifact\n",
    "import mlflow\n",
    "from sklearn.metrics import precision_recall_fscore_support, accuracy_score, matthews_corrcoef\n",
    "\n",
    "#create a hyperparameters dictionary \n",
    "dim = [2, 5, 35, 50, 100, 300]  \n",
    "cycles = [5, 10, 15, 20, 50] #stop if loss does not decrease\n",
    "window = range(1, 9, 1)  \n",
    "negative = range(0, 20, 5) # 0 leads to not having the syn1neg property. \n",
    "exponent =  [-1, -0.75, 0, 0.75, 1] \n",
    "alpha = [0.015] # range(0.015, 0.1, 0.005)\n",
    "min_alpha = [0.00001]  # range(0.00001, 0.001, 0.0001)\n",
    "hs = [0, 1]\n",
    "sample = [0]  # range(0, 0.00001, 0.00001) \n",
    "# threshold = range(1e-5, 1e-4, 1e-5) Not needed for now\n",
    "\n",
    "#train the model \n",
    "#for a,b,c in itertools.combination(cycles, dim...)\n",
    "i = 0\n",
    "for c in cycles:\n",
    "    for d in dim:\n",
    "        for w in window:\n",
    "            for n in negative:\n",
    "                for e in exponent:\n",
    "                    for a in alpha:\n",
    "                        for m in min_alpha:\n",
    "                            for s in sample:\n",
    "                                for h in hs:\n",
    "\n",
    "                                    i+=1\n",
    "                                    with mlflow.start_run(nested=True):\n",
    "\n",
    "                                        # Log params to mlflow\n",
    "                                        log_param(\"cycles\", c)\n",
    "                                        log_param(\"dim\", d)\n",
    "                                        log_param(\"window\", w)\n",
    "                                        log_param(\"negative\", n)\n",
    "                                        log_param(\"exponent\", e)\n",
    "                                        log_param(\"alpha\", a)\n",
    "                                        log_param(\"min_alpha\", m)\n",
    "                                        log_param(\"sample\", s)\n",
    "                                        log_param(\"hs\", h)\n",
    "\n",
    "                                        # curr_model = training(sentences_series_np_train, cycles = c, dim = d, window = w, sample = s, \n",
    "                                        #                     negative = n, hs = h, exponent = e, alpha = a, min_alpha = m)\n",
    "\n",
    "                                        curr_model = training(sentences_series_np_train, 5, 5, 9, 0, 5, 0, 0.75, 0.025, 0.0001)\n",
    "                                    \n",
    "                                        #metrics for each combination \n",
    "                                        prevs = predict(curr_model, sentences_series_np_train[:100], 3e-5, verbose1 = False, verbose2 = False)\n",
    "                                        precision, recall, fscore, _ = precision_recall_fscore_support(y_train[:100], prevs, average='micro')\n",
    "                                        mcc = matthews_corrcoef(y_train[:100], prevs)\n",
    "                                        log_param(\"accuracy\", accuracy_score(y_train[:100], prevs))\n",
    "                                        log_param(\"precision\", precision)\n",
    "                                        log_param(\"recall\", recall)\n",
    "                                        log_param(\"f1\", fscore)\n",
    "                                        log_param(\"mcc\", mcc)\n",
    "                                        \n",
    "\n",
    "                                        curr_model.save(\"trained_model.w2v\")\n",
    "                                        log_artifact(\"trained_model.w2v\")\n",
    "                                        os.remove(\"trained_model.w2v\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f40b9921",
   "metadata": {},
   "outputs": [],
   "source": [
    "#option 1\n",
    "#python 3transfers_word2vec_matrix_originalcols_saving.py &> results.txt\n",
    "\n",
    "\n",
    "#option 2\n",
    "#def fprint(output):\n",
    "#    print output\n",
    "#    with open(\"somefile.txt\", \"a\") as f:\n",
    "#        f.write(\"{}\\n\".format(output))\n",
    "\n",
    "\n",
    "#option 3\n",
    "#from contextlib import redirect_stdout\n",
    "\n",
    "#with open('results.log', 'w') as f:\n",
    "#    with redirect_stdout(f):\n",
    "#        print('generating')\n",
    "#        # the rest of your code or main function goes here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dabb28cf",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#apply the prediction method for the test set\n",
    "predict(model, X_test_np[:1000], 0.5, verbose1=True, verbose2=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "180bfb6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert true values in train set to binary\n",
    "y_test = [1 if elem == \"fraud\" else 0 for elem in y_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54ce881a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#true values\n",
    "set(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0a0825f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#metrics for the test set \n",
    "from imblearn.metrics import geometric_mean_score\n",
    "from sklearn.metrics import precision_recall_fscore_support, accuracy_score, matthews_corrcoef\n",
    "\n",
    "trs = y_test[:10]\n",
    "prevs = predict(model, X_test_np.values[:10], 0.1, verbose1=False, verbose2=True)\n",
    "\n",
    "#accuracy\n",
    "accuracy = accuracy_score(trs, prevs)\n",
    "print('accuracy: {}'.format(accuracy))\n",
    "\n",
    "#precision, recall, f-score\n",
    "precision, recall, fscore, support = precision_recall_fscore_support(trs,prevs, average='micro')\n",
    "\n",
    "#precision - ratio tp / (tp + fp) - ability not to label a negative sample as positive\n",
    "print('precision: {}'.format(precision))\n",
    "\n",
    "#recall - ratio tp / (tp + fn) - ability to find all the positive samples - best is 1, worst is 0\n",
    "print('recall: {}'.format(recall))\n",
    "\n",
    "#fscore - weighted harmonic mean of the precision and recall - best is 1, worst is 0\n",
    "print('fscore: {}'.format(fscore))\n",
    "\n",
    "#matthews correlation coefficient - measure of the quality of binary classifications\n",
    "#can be used even if the classes are of very different sizes - is in essence a correlation coefficient between -1 and +1\n",
    "#+1 means perfect prediction, 0 an average random prediction, -1 an inverse prediction\n",
    "mcc = matthews_corrcoef(trs, prevs)\n",
    "print('mcc: {}'.format(mcc))\n",
    "\n",
    "#g-mean - squared root of the product of the sensitivity and specificity - best is 1, worst is 0\n",
    "print('G-mean:', (\n",
    "    geometric_mean_score(trs, prevs, average='micro')))\n",
    "\n",
    "#specificity - ability to predict true negatives of each available category - recall of the negative class\n",
    "specifity = tn / (tn + fp)\n",
    "print('specifity: {}'.format(specifity))\n",
    "\n",
    "#sensitivity - ability to predict true positives of each available category = recall\n",
    "\n",
    "#confusion matrix    \n",
    "print(confusion_matrix(trs, prevs))\n",
    "\n",
    "#true positives, false positives, true negatives, false negatives\n",
    "tn, fp, fn, tp = confusion_matrix(trs, prevs).ravel()\n",
    "\n",
    "#true negatives\n",
    "print('true negatives: {}'.format(tn))\n",
    "\n",
    "#false positives\n",
    "print('false positives: {}'.format(fp))\n",
    "\n",
    "#false negatives\n",
    "print('false negatives: {}'.format(fn))\n",
    "\n",
    "#false positives\n",
    "print('true positives: {}'.format(tp))\n",
    "\n",
    "#error rate\n",
    "error_rate = 1 - accuracy\n",
    "print('error rate: {}'.format(error_rate))\n",
    "\n",
    "#precision-recall curve - compute precision-recall pairs for different probability thresholds\n",
    "print(precision_recall_curve(trs, prevs))\n",
    "\n",
    "#roc curve\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import metrics\n",
    "\n",
    "fpr, tpr, thresholds = metrics.roc_curve(trs, prevs)\n",
    "\n",
    "roc_auc = metrics.auc(fpr, tpr)\n",
    "\n",
    "display = metrics.RocCurveDisplay(fpr=fpr, tpr=tpr, roc_auc=roc_auc, estimator_name='Word2vec')\n",
    "display.plot()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caa92c47",
   "metadata": {},
   "outputs": [],
   "source": [
    "#save the threshold and the ratio in a list\n",
    "def thres_numberf():\n",
    "    threshold = [0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1]\n",
    "    ratio = []\n",
    "\n",
    "    for thresh in threshold:\n",
    "        #assign to a current variable the prediction result\n",
    "        curr = predict(model, X_test_np.values, thresh, verbose1=False, verbose2=False)\n",
    "\n",
    "        #ratio of the number of detected frauds per the number of real frauds\n",
    "        ratio.append(sum(curr) / sum(y_test))\n",
    "\n",
    "    return threshold, ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff05a519",
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot the threshold vs ratio (number of detected frauds/number of real frauds)\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#assign the values to plot\n",
    "thresh, goal = thres_numberf()\n",
    "\n",
    "plt.plot(thresh, goal, 'b-')\n",
    "plt.xlabel('threshold')\n",
    "plt.ylabel('#frauds detected/#real frauds')\n",
    "\n",
    "#save to csv\n",
    "thresh_ratio = np.column_stack((thresh.flatten(), goal.flatten()))\n",
    "np.savetxt('threshold_ratio.csv', thresh_ratio, goal, delimiter=',')\n",
    "\n",
    "#log scale\n",
    "plt.yscale('log')\n",
    "\n",
    "plt.show()\n",
    "\n",
    "#plt.savefig()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d5ad885",
   "metadata": {},
   "outputs": [],
   "source": [
    "#---------------------------------------------------density matrix------------------------------------------------------------#\n",
    "\n",
    "#product of the matrices\n",
    "mproduct = np.matmul(m1, m2.T)\n",
    "mproduct.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ecaf72a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#apply softmax to obtain a matrix with conditional probabilities\n",
    "conditional_probs = softmax(mproduct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b91ba786",
   "metadata": {},
   "outputs": [],
   "source": [
    "#save density matrix \n",
    "\n",
    "#option 1\n",
    "#np.savetxt('3transfers_word2vec_matrix_originalcols_density_matrix.csv', conditional_probs, delimiter = ',')\n",
    "\n",
    "#option 2\n",
    "#pd.DataFrame(conditional_probs).to_csv(\"3transfers_word2vec_matrix_originalcols_density_matrix.csv\", header = None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac893603",
   "metadata": {},
   "outputs": [],
   "source": [
    "#confirm that softmax sum is 1\n",
    "conditional_probs.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d7c6465",
   "metadata": {},
   "outputs": [],
   "source": [
    "#maximum value of the conditional probabilities\n",
    "np.max(conditional_probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed7fd863",
   "metadata": {},
   "outputs": [],
   "source": [
    "#minimum value of the conditional probabilities\n",
    "np.min(conditional_probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "963e0b6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#visualize the 1st 10 lines and the last 10 lines of the density matrix\n",
    "\n",
    "#1st 10 lines\n",
    "conditional_aux = conditional_probs[:10].copy()\n",
    "\n",
    "#last 10 lines\n",
    "conditional_aux = np.concatenate((conditional_aux, conditional_probs[-10:].copy()))\n",
    "\n",
    "#display setting\n",
    "from IPython.core.display import HTML\n",
    "\n",
    "display(HTML(\"<style>pre { white-space: pre !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42cd964d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#shape of the compressed density matrix \n",
    "conditional_aux.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac91f8aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#print the compressed conditional probabilities matrix\n",
    "print('\\n'.join(['\\t'.join([str(cell) for cell in row]) for row in conditional_aux]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4daf4fdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#-------------------------------------------------eigenvalues-----------------------------------------------------------------#\n",
    "\n",
    "#eingenvalues of the square matrix before softmax (w has the eigenvalues and v the eigenvectors)\n",
    "w, v = np.linalg.eig(mproduct)  #mproduct is an array of arrays\n",
    "\n",
    "#separate real and imaginary parts of the eigenvalues\n",
    "x = w.real  #array\n",
    "\n",
    "y = w.imag  #array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acea2653",
   "metadata": {},
   "outputs": [],
   "source": [
    "#eigenvalues\n",
    "w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94c25cf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#eigenvalues plot for the several vector sizes - square of the numbers in mproduct (product of the matrices before softmax)  \n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "#plot complex numbers\n",
    "def plot_eigenvalues(model, vec_size):\n",
    "    x_r = np.square(x)\n",
    "    y_i = np.square(y)\n",
    "\n",
    "    plt.plot(x_r, 'b-')\n",
    "    plt.ylabel('Imaginary')\n",
    "    plt.xlabel('Real')\n",
    "\n",
    "    #plt.ylim(0, 250e6)\n",
    "    #plt.xlim(0, 15)\n",
    "\n",
    "    fig_name = \"vec_size_\" + vec_size + \"_best_comb.png\"\n",
    "    plt.savefig(fig_name)\n",
    "\n",
    "    return x_r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73943d5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#train the model with the optimized parameters for different vector sizes  \n",
    "\n",
    "#dictionary of results\n",
    "result_dic = {}\n",
    "\n",
    "#vector sizes to try\n",
    "vec_sizes = [3, 4]\n",
    "\n",
    "#train the model\n",
    "for curr_vec in vec_sizes:\n",
    "    curr_model = w2v.Word2Vec(\n",
    "        sg=1,  #skip-gram\n",
    "        workers=multiprocessing.cpu_count(),  #use all cores\n",
    "        vector_size=curr_vec,  #dimension of the embedding space\n",
    "        window=9,  #words befores and after the center word\n",
    "        sample=0,  #whithout subsampling\n",
    "        min_count=1,  #use every word\n",
    "        negative=5,  #noise-words\n",
    "        hs=0,  #negative sampling\n",
    "        ns_exponent=0,  #exponent to shape negative sampling\n",
    "        alpha=0.025,  #initial learning rate\n",
    "        min_alpha=0.0001  #final learning rate\n",
    "    )\n",
    "\n",
    "    #vocabulary creation\n",
    "    model.build_vocab(sentences_series)\n",
    "\n",
    "    #model training\n",
    "    model.train(sentences_series, epochs=5, total_examples=model.corpus_count, compute_loss=True,\n",
    "                callbacks=[callback()])\n",
    "\n",
    "    #save results in the dictionary \n",
    "    result_dic[curr_vec_size] = plot_eigenvalues(curr_model, curr_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "873e5396",
   "metadata": {},
   "outputs": [],
   "source": [
    "#option 1 - export result_dic and plot in excel\n",
    "#convert the dictionary to dataframe\n",
    "result_dic = pd.DataFrame(data=result_dic, index=[0])\n",
    "result_dic = (result_dic.T)\n",
    "result_dic.to_excel('3transfers_word2vec_matrix_originalcols_eingenvalues.xlsx')\n",
    "\n",
    "#option 2 - plot result_dic with matplotlib\n",
    "plt.plot(list(result_dic.keys()), list(result_dic.values()))\n",
    "plt.legend(['3', '4'], loc='upper left')\n",
    "plt.savefig('3transfers_word2vec_matrix_originalcols_eingenvalues.png')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "issueVenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "e06c1558c1cf6512bb6dc6fbbfb1a8e15d917071b508e44b99cfbe65933af948"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
